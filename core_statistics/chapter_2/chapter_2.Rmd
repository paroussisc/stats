---
title: "Chapter 2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Statistical Models and Inference

This document contains some of the more useful notes from Chapter 2.

#Random effects:

Random variables in a model that are not associated with the independent random variability of single observations, are termed random effects. It allows us to decompose randomness into multiple components, most commonly introducing individual variability in the model in a way similar to Bayesian hierarchical modelling. When the pdfs can't be written down in closed form, we use the fact that the $f(y|b)$ are independent, which we exploit when calculating the likelihood using $f(y, b) = f(y | b)f(b).$

We usually need to integrate out the random effects, which we can do using stochastic simulation or (Laplace) approximation.

#Inferential questions:

* What values for $\theta$ are most consistent with $y$? (point estimation)
* Is some prespecified restriction on $\theta$ consistent with $y$? (hypothesis testing)
* What ranges of values of $\theta$ are consistent with $y$? (interval estimation)
* Is the model consistent with the data for any values of $\theta$ at all? (model checking)

Point estimation:

MLE is unbiased and achieves the Cramer-Rao lower bound of the variance.

Likelihood ratio:

If comparing restrictions on $\theta$, then the parameter vector that maximised the likelihood within said restriction is the one that is used for the likelihood ratio test.

In general, for testing the null hypothesis that $\theta$ is restricted vs unrestricted we can use the fact that given some regularity conditions, in the large same limit (see Chapter 4) we have:

$$2\{\log(f_{\hat{\theta}}(y))-\log(f_{\hat{\theta_0}}(y))\}\sim \chi^2_r$$
for $r$ restrictions on $\theta$.

In general for $p$-values: $p$ is the probability of obtaining a test statistic at least as favourable to \textbf{$H_1$} as that observed, if \textbf{$H_0$} is true.

The Neyman-Pearson Lemma states that the likelihood ratio test is the most powerful (best at correctly rejecting null hypothesis) test possible.

Model checking:

* qq-plots to check theoretical quantiles vs observed quantiles of marginal distribution of elements of $y$.
* Standardised residuals should appear independent with constant variance if the model is "correct".

K-L divergence is the expected difference in the log likelihood ratio between the true model and the model under construction - gets used to derive AIC and cross-validation score.

Bayesian methods use BIC and DIC. BIC has troubles since it is not always clear how to count the number of free parameters, as well as BIC's dependence on knowledge of posterior nodes, which are not directly accessible via simulation. DIC uses effective degrees of freedom and overcomes this, however its use cannot be justified if effective number of parameters is not small compared to the number of data in $y$.

# Useful single-parameter normal results

If we know $\hat{\theta} \sim N(\theta, \sigma_\theta)$, where $\sigma_\theta$ is known but $\theta$ is not, we can test $H_0$:$\theta=\theta_0$ vs $H_1$:$\theta\neq \theta_0$ with the test statistic:

$$\frac{\hat{\theta}-\theta_0}{\sigma_\theta}$$
which is $N(0,1)$ distributed if $H_0$ is true. So we can compute the $p$-value using:
```
z.obs <- (theta.hat - theta.0)/sigma
pnorm(abs(z.obs),lower.tail=FALSE) + pnorm(-abs(z.obs))
pnorm(-abs(z.obs))*2 ## use symmetry of N(0,1)
pchisq(z.obs^2,lower.tail=FALSE,df=1) ## equivalent
```

and for estimated variance:

```
t.obs <- (theta.hat - theta.0)/(const*sigma.hat)
pt(-abs(z.obs),df=k)*2 ## use symmetry of t_k
```

For some known constant. Of course we also have the confidence intervals:

```
theta.hat + qnorm(c(.025,.975))*sigma
theta.hat + qt(c(.025,.975),df=k)*const*sigma.hat
```

















