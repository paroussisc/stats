---
title: "Chapter 7"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Jamesâ€“Stein Estimation and Ridge Regression

# James-Stein Estimator

- Always achieves lower MSE than MLE as per the James-Stein Theorem for 3 or more unrelated parameters.
- Empirical mean and standard deviation used for prior, then standard Bayesian update for Gaussians gives, for $\hat{M}$ empirical unbiased prior, $\hat{B}$ the unbiased posterior estimate of $B=\frac{A}{A+1}$ ($A$ being the prior standard deviation):

$$\hat{\mu}^{JS}_i = \hat{M} + \hat{B}(x_i - \hat{M})$$ for $i=1, 2,..., N,$ or equivalently in vector form.

This is Empirical Bayes again. It won't perform as well as Bayes Rule but the increased risk (mean squared error) isn't that big.

# Ridge regression

- First we can standardise Linear Regression so that the columns of the model matrix have mean 0 and sum of squares 1, so that the coefficients are on the same scale.
- Ridge Regression shrinks the coefficient vector $\hat{\beta}$.
- It has Bayesian justificaiton - it's linear regression with an increased prior belief that the coefficients lie near 0.
- Currently no optimality theory for shrinkage estimation.
- Shrinkage estimators are not good for cases that are genuinely outstanding. Why should an estimate for one player go up or down depending on another? Direct vs indirect evidence is discussed more in Chapter 15.

